{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "ImageGeneratorDCGAN.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maxiaotian520/image_generator/blob/master/ImageGeneratorDCGAN_modify02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "267d5504009eca2b809a2691131366baebe98436",
        "id": "B8nRWthodt0I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import datetime\n",
        "import random\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import savetxt\n",
        "import pandas as pd\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvUj_IxLeZSO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ee61e232-ecc0-4523-ccfe-d2f4a3dd8f88"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "fc376df46433261bfeb643a95793718a9d969ed1",
        "id": "nXVubwxBdt0T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator(z, output_channel_dim, training):\n",
        "    with tf.variable_scope(\"generator\", reuse= not training):\n",
        "        \n",
        "        # 8x8x1024\n",
        "        fully_connected = tf.layers.dense(z, 8*8*1024)\n",
        "        fully_connected = tf.reshape(fully_connected, (-1, 8, 8, 1024))\n",
        "        fully_connected = tf.nn.leaky_relu(fully_connected)\n",
        "\n",
        "        # 8x8x1024 -> 16x16x512\n",
        "        trans_conv1 = tf.layers.conv2d_transpose(inputs=fully_connected,\n",
        "                                                 filters=512,\n",
        "                                                 kernel_size=[5,5],\n",
        "                                                 strides=[2,2],\n",
        "                                                 padding=\"SAME\",\n",
        "                                                 kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\n",
        "                                                 name=\"trans_conv1\")\n",
        "        batch_trans_conv1 = tf.layers.batch_normalization(inputs = trans_conv1,\n",
        "                                                          training=training,\n",
        "                                                          epsilon=EPSILON,\n",
        "                                                          name=\"batch_trans_conv1\")\n",
        "        trans_conv1_out = tf.nn.leaky_relu(batch_trans_conv1,\n",
        "                                           name=\"trans_conv1_out\")\n",
        "        \n",
        "        # 16x16x512 -> 32x32x256\n",
        "        trans_conv2 = tf.layers.conv2d_transpose(inputs=trans_conv1_out,\n",
        "                                                 filters=256,\n",
        "                                                 kernel_size=[5,5],\n",
        "                                                 strides=[2,2],\n",
        "                                                 padding=\"SAME\",\n",
        "                                                 kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\n",
        "                                                 name=\"trans_conv2\")\n",
        "        batch_trans_conv2 = tf.layers.batch_normalization(inputs = trans_conv2,\n",
        "                                                          training=training,\n",
        "                                                          epsilon=EPSILON,\n",
        "                                                          name=\"batch_trans_conv2\")\n",
        "        trans_conv2_out = tf.nn.leaky_relu(batch_trans_conv2,\n",
        "                                           name=\"trans_conv2_out\")\n",
        "        \n",
        "        # 32x32x256 -> 64x64x128\n",
        "        trans_conv3 = tf.layers.conv2d_transpose(inputs=trans_conv2_out,\n",
        "                                                 filters=128,\n",
        "                                                 kernel_size=[5,5],\n",
        "                                                 strides=[2,2],\n",
        "                                                 padding=\"SAME\",\n",
        "                                                 kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\n",
        "                                                 name=\"trans_conv3\")\n",
        "        batch_trans_conv3 = tf.layers.batch_normalization(inputs = trans_conv3,\n",
        "                                                          training=training,\n",
        "                                                          epsilon=EPSILON,\n",
        "                                                          name=\"batch_trans_conv3\")\n",
        "        trans_conv3_out = tf.nn.leaky_relu(batch_trans_conv3,\n",
        "                                           name=\"trans_conv3_out\")\n",
        "        \n",
        "        # 64x64x128 -> 128x128x64\n",
        "        trans_conv4 = tf.layers.conv2d_transpose(inputs=trans_conv3_out,\n",
        "                                                 filters=64,\n",
        "                                                 kernel_size=[5,5],\n",
        "                                                 strides=[2,2],\n",
        "                                                 padding=\"SAME\",\n",
        "                                                 kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\n",
        "                                                 name=\"trans_conv4\")\n",
        "        batch_trans_conv4 = tf.layers.batch_normalization(inputs = trans_conv4,\n",
        "                                                          training=training,\n",
        "                                                          epsilon=EPSILON,\n",
        "                                                          name=\"batch_trans_conv4\")\n",
        "        trans_conv4_out = tf.nn.leaky_relu(batch_trans_conv4,\n",
        "                                           name=\"trans_conv4_out\")\n",
        "        \n",
        "        # 128x128x64 -> 128x128x3\n",
        "        logits = tf.layers.conv2d_transpose(inputs=trans_conv4_out,\n",
        "                                            filters=3,\n",
        "                                            kernel_size=[5,5],\n",
        "                                            strides=[1,1],\n",
        "                                            padding=\"SAME\",\n",
        "                                            kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\n",
        "                                            name=\"logits\")\n",
        "        out = tf.tanh(logits, name=\"out\")\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "ba53a4bb09dbcd57d3e3392f74ccd054ecf23ecb",
        "id": "SSIfMkOEdt0Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discriminator(x, reuse):\n",
        "    with tf.variable_scope(\"discriminator\", reuse=reuse): \n",
        "        \n",
        "        # 128*128*3 -> 64x64x64 \n",
        "        conv1 = tf.layers.conv2d(inputs=x,\n",
        "                                 filters=64,\n",
        "                                 kernel_size=[5,5],\n",
        "                                 strides=[2,2],\n",
        "                                 padding=\"SAME\",\n",
        "                                 kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\n",
        "                                 name='conv1')\n",
        "        batch_norm1 = tf.layers.batch_normalization(conv1,\n",
        "                                                    training=True,\n",
        "                                                    epsilon=EPSILON,\n",
        "                                                    name='batch_norm1')\n",
        "        conv1_out = tf.nn.leaky_relu(batch_norm1,\n",
        "                                     name=\"conv1_out\")\n",
        "        \n",
        "        # 64x64x64-> 32x32x128 \n",
        "        conv2 = tf.layers.conv2d(inputs=conv1_out,\n",
        "                                 filters=128,\n",
        "                                 kernel_size=[5, 5],\n",
        "                                 strides=[2, 2],\n",
        "                                 padding=\"SAME\",\n",
        "                                 kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\n",
        "                                 name='conv2')\n",
        "        batch_norm2 = tf.layers.batch_normalization(conv2,\n",
        "                                                    training=True,\n",
        "                                                    epsilon=EPSILON,\n",
        "                                                    name='batch_norm2')\n",
        "        conv2_out = tf.nn.leaky_relu(batch_norm2,\n",
        "                                     name=\"conv2_out\")\n",
        "        \n",
        "        # 32x32x128 -> 16x16x256  \n",
        "        conv3 = tf.layers.conv2d(inputs=conv2_out,\n",
        "                                 filters=256,\n",
        "                                 kernel_size=[5, 5],\n",
        "                                 strides=[2, 2],\n",
        "                                 padding=\"SAME\",\n",
        "                                 kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\n",
        "                                 name='conv3')\n",
        "        batch_norm3 = tf.layers.batch_normalization(conv3,\n",
        "                                                    training=True,\n",
        "                                                    epsilon=EPSILON,\n",
        "                                                    name='batch_norm3')\n",
        "        conv3_out = tf.nn.leaky_relu(batch_norm3,\n",
        "                                     name=\"conv3_out\")\n",
        "        \n",
        "        # 16x16x256 -> 16x16x512\n",
        "        conv4 = tf.layers.conv2d(inputs=conv3_out,\n",
        "                                 filters=512,\n",
        "                                 kernel_size=[5, 5],\n",
        "                                 strides=[1, 1],\n",
        "                                 padding=\"SAME\",\n",
        "                                 kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\n",
        "                                 name='conv4')\n",
        "        batch_norm4 = tf.layers.batch_normalization(conv4,\n",
        "                                                    training=True,\n",
        "                                                    epsilon=EPSILON,\n",
        "                                                    name='batch_norm4')\n",
        "        conv4_out = tf.nn.leaky_relu(batch_norm4,\n",
        "                                     name=\"conv4_out\")\n",
        "        \n",
        "        # 16x16x512 -> 8x8x1024\n",
        "        conv5 = tf.layers.conv2d(inputs=conv4_out,\n",
        "                                filters=1024,\n",
        "                                kernel_size=[5, 5],\n",
        "                                strides=[2, 2],\n",
        "                                padding=\"SAME\",\n",
        "                                kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\n",
        "                                name='conv5')\n",
        "        batch_norm5 = tf.layers.batch_normalization(conv5,\n",
        "                                                    training=True,\n",
        "                                                    epsilon=EPSILON,\n",
        "                                                    name='batch_norm5')\n",
        "        conv5_out = tf.nn.leaky_relu(batch_norm5,\n",
        "                                     name=\"conv5_out\")\n",
        "\n",
        "        flatten = tf.reshape(conv5_out, (-1, 8*8*1024))\n",
        "        logits = tf.layers.dense(inputs=flatten,\n",
        "                                 units=1,\n",
        "                                 activation=None)\n",
        "        out = tf.sigmoid(logits)\n",
        "        return out, logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8ef5bbb8e4d157577f1b15600aa64cb40289a754",
        "id": "zAB6e5Zcdt0f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_loss(input_real, input_z, output_channel_dim):\n",
        "    g_model = generator(input_z, output_channel_dim, True)\n",
        "\n",
        "    noisy_input_real = input_real + tf.random_normal(shape=tf.shape(input_real),\n",
        "                                                     mean=0.0,\n",
        "                                                     stddev=random.uniform(0.0, 0.1),\n",
        "                                                     dtype=tf.float32)\n",
        "    \n",
        "    d_model_real, d_logits_real = discriminator(noisy_input_real, reuse=False)\n",
        "    d_model_fake, d_logits_fake = discriminator(g_model, reuse=True)\n",
        "    \n",
        "    d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real,\n",
        "                                                                         labels=tf.ones_like(d_model_real)*random.uniform(0.9, 1.0)))\n",
        "    d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,\n",
        "                                                                         labels=tf.zeros_like(d_model_fake)))\n",
        "    d_loss = tf.reduce_mean(0.5 * (d_loss_real + d_loss_fake))\n",
        "    g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,\n",
        "                                                                    labels=tf.ones_like(d_model_fake)))\n",
        "    return d_loss, g_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "01820b197de1b6d0ca39592043308557d941937a",
        "id": "R__LaaVKdt0k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_optimizers(d_loss, g_loss):\n",
        "    t_vars = tf.trainable_variables()\n",
        "    g_vars = [var for var in t_vars if var.name.startswith(\"generator\")]\n",
        "    d_vars = [var for var in t_vars if var.name.startswith(\"discriminator\")]\n",
        "    \n",
        "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "    gen_updates = [op for op in update_ops if op.name.startswith('generator')]\n",
        "    \n",
        "    with tf.control_dependencies(gen_updates):\n",
        "        d_train_opt = tf.train.AdamOptimizer(learning_rate=LR_D, beta1=BETA1).minimize(d_loss, var_list=d_vars)\n",
        "        g_train_opt = tf.train.AdamOptimizer(learning_rate=LR_G, beta1=BETA1).minimize(g_loss, var_list=g_vars)  \n",
        "    return d_train_opt, g_train_opt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BKiT1vDdt0o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_inputs(real_dim, z_dim):\n",
        "    inputs_real = tf.placeholder(tf.float32, (None, *real_dim), name='inputs_real')\n",
        "    inputs_z = tf.placeholder(tf.float32, (None, z_dim), name=\"input_z\")\n",
        "    learning_rate_G = tf.placeholder(tf.float32, name=\"lr_g\")\n",
        "    learning_rate_D = tf.placeholder(tf.float32, name=\"lr_d\")\n",
        "    return inputs_real, inputs_z, learning_rate_G, learning_rate_D"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "3a7808bacf25966ab5a8ca06e2e1075ac8eeb662",
        "id": "RwmTdg0hdt0t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_samples(sample_images, name, epoch):\n",
        "    figure, axes = plt.subplots(1, len(sample_images), figsize = (IMAGE_SIZE, IMAGE_SIZE))\n",
        "    for index, axis in enumerate(axes):\n",
        "        axis.axis('off')\n",
        "        #flatten() change 3d array into 1d\n",
        "        image_array = sample_images[index].flatten()\n",
        "        if epoch == 50:\n",
        "          !kill -9 -1\n",
        "          array_sum = []\n",
        "          array_sum = np.concatenate((image_array, array_sum), axis=0)\n",
        "        else:\n",
        "          print(epoch)\n",
        "          print(image_array.size)\n",
        "          print(image_array.shape)\n",
        "          print(image_array)\n",
        "    if epoch == 300:\n",
        "    \t#save to csv file \n",
        "      #pd.DataFrame(image_array).to_csv(\"/content/drive/My Drive/Colab Notebooks/image_generator/output.csv\")\n",
        "      savetxt('/content/drive/My Drive/Colab Notebooks/image_generator/output.csv', array_sum, delimiter=' ')\n",
        "      #print(array_sum.size)\n",
        "      #print(array_sum.shape)\n",
        "      #print(array_sum)\n",
        "        #axis.imshow(image_array)\n",
        "        #实现array到image的转换\n",
        "        #image = Image.fromarray(image_array)\n",
        "        #image.save(name+\"_\"+str(epoch)+\"_\"+str(index)+\".png\") \n",
        "    #plt.savefig(name+\"_\"+str(epoch)+\".png\", bbox_inches='tight', pad_inches=0)\n",
        "    #plt.show()\n",
        "    #plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXLGmyBUdt0x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(sess, input_z, out_channel_dim, epoch):\n",
        "    example_z = np.random.uniform(-1, 1, size=[SAMPLES_TO_SHOW, input_z.get_shape().as_list()[-1]])\n",
        "    samples = sess.run(generator(input_z, out_channel_dim, False), feed_dict={input_z: example_z})\n",
        "    sample_images = [((sample + 1.0) * 127.5).astype(np.uint8) for sample in samples]\n",
        "    show_samples(sample_images, OUTPUT_DIR + \"samples\", epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "0ed8f1c378f936ac81ae89b35d5b6914cd6efbbb",
        "id": "7ecFv9S9dt01",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def summarize_epoch(epoch, duration, sess, d_losses, g_losses, input_z, data_shape):\n",
        "    minibatch_size = int(data_shape[0]//BATCH_SIZE)\n",
        "    # print(\"Epoch {}/{}\".format(epoch, EPOCHS),\n",
        "    #       \"\\nDuration: {:.5f}\".format(duration),\n",
        "    #       \"\\nD Loss: {:.5f}\".format(np.mean(d_losses[-minibatch_size:])),\n",
        "    #       \"\\nG Loss: {:.5f}\".format(np.mean(g_losses[-minibatch_size:])))\n",
        "    # fig, ax = plt.subplots()\n",
        "    # plt.plot(d_losses, label='Discriminator', alpha=0.6)\n",
        "    # plt.plot(g_losses, label='Generator', alpha=0.6)\n",
        "    # plt.title(\"Losses\")\n",
        "    # plt.legend()\n",
        "    # plt.savefig(OUTPUT_DIR + \"losses_\" + str(epoch) + \".png\")\n",
        "    # plt.show()\n",
        "    # plt.close()\n",
        "    test(sess, input_z, data_shape[3], epoch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FIugUZqdt05",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_batches(data):\n",
        "    batches = []\n",
        "    for i in range(int(data.shape[0]//BATCH_SIZE)):\n",
        "        batch = data[i * BATCH_SIZE:(i + 1) * BATCH_SIZE]\n",
        "        augmented_images = []\n",
        "        for img in batch:\n",
        "            image = Image.fromarray(img)\n",
        "            if random.choice([True, False]):\n",
        "                image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "            augmented_images.append(np.asarray(image))\n",
        "        batch = np.asarray(augmented_images)\n",
        "        normalized_batch = (batch / 127.5) - 1.0\n",
        "        batches.append(normalized_batch)\n",
        "    return batches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1emVMLFdt09",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(get_batches, data_shape, checkpoint_to_load=None):\n",
        "    input_images, input_z, lr_G, lr_D = model_inputs(data_shape[1:], NOISE_SIZE)\n",
        "    d_loss, g_loss = model_loss(input_images, input_z, data_shape[3])\n",
        "    d_opt, g_opt = model_optimizers(d_loss, g_loss)\n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        epoch = 0\n",
        "        iteration = 0\n",
        "        d_losses = []\n",
        "        g_losses = []\n",
        "        \n",
        "        for epoch in range(EPOCHS):        \n",
        "            epoch += 1\n",
        "            start_time = time.time()\n",
        "\n",
        "            for batch_images in get_batches:\n",
        "                iteration += 1\n",
        "                batch_z = np.random.uniform(-1, 1, size=(BATCH_SIZE, NOISE_SIZE))\n",
        "                _ = sess.run(d_opt, feed_dict={input_images: batch_images, input_z: batch_z, lr_D: LR_D})\n",
        "                _ = sess.run(g_opt, feed_dict={input_images: batch_images, input_z: batch_z, lr_G: LR_G})\n",
        "                d_losses.append(d_loss.eval({input_z: batch_z, input_images: batch_images}))\n",
        "                g_losses.append(g_loss.eval({input_z: batch_z}))\n",
        "\n",
        "            summarize_epoch(epoch, time.time()-start_time, sess, d_losses, g_losses, input_z, data_shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "d857345ed6c3ba7e4f614fa90b5ca42adb9917cf",
        "id": "haT4YG4_dt1B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Paths\n",
        "INPUT_DATA_DIR = \"/content/drive/My Drive/Colab Notebooks/image_generator/\" # Path to the folder with input images. For more info check simspons_dataset.txt\n",
        "OUTPUT_DIR = './{date:%Y-%m-%d_%H:%M:%S}/'.format(date=datetime.datetime.now())\n",
        "if not os.path.exists(OUTPUT_DIR):\n",
        "    os.makedirs(OUTPUT_DIR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JtZdl_adt1F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparameters\n",
        "IMAGE_SIZE = 128\n",
        "NOISE_SIZE = 100\n",
        "LR_D = 0.00004\n",
        "LR_G = 0.0004\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 300\n",
        "BETA1 = 0.5\n",
        "WEIGHT_INIT_STDDEV = 0.02\n",
        "EPSILON = 0.00005\n",
        "SAMPLES_TO_SHOW = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GCmrEE3e2T4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def readcsv(filename):\n",
        "  data = pd.read_csv(filename) #Please add four spaces here before this line\n",
        "  return(np.array(data)) #Please add four spaces here before this line"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "1ee6349afccb4d2f29f36d6605dd2f156350821a",
        "scrolled": false,
        "id": "wjcKFrFYdt1K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "67e6e925-b120-4739-a4a1-8cc8cecffd26"
      },
      "source": [
        "# Training\n",
        "#input_images = np.asarray([np.asarray(Image.open(\"/content/drive/My Drive/Colab Notebooks/AI_Lab/image_generator/1.png\").resize((IMAGE_SIZE, IMAGE_SIZE))) for file in glob(INPUT_DATA_DIR + '*')])\n",
        "\n",
        "#print (input_images.shape)\n",
        "#print (input_images.size) #294912\n",
        "#!kill -9 -1\n",
        "#np.random.shuffle(input_images)\n",
        "#print (\"==========================\")\n",
        "#sample_images = random.sample(list(input_images), SAMPLES_TO_SHOW)\n",
        "#sample_images = list(input_images)\n",
        "#print(sample_images)\n",
        "Input_data = readcsv(\"/content/drive/My Drive/Colab Notebooks/image_generator/data02.csv\")   #data02 295098   data02(295098)-186=293912 这里data02已经减去了这么多，为了方便reshape矩阵\n",
        "#Input_data = np.reshape(Input_data, (128, 3)) #这里还是要用它原来的维度 (6, 128, 128, 3)\n",
        "#Input_data = np.reshape(Input_data, (128, 128, 3))\n",
        "Input_data = np.reshape(Input_data, (294912,))\n",
        "#print(Input_data.size)\n",
        "#print(Input_data.shape)\n",
        "#print(Input_data)\n",
        "Input_data = np.reshape(Input_data, (6, 128, 128, 3))\n",
        "#print(Input_data.size)\n",
        "#Input_data = np.transpose(Input_data)\n",
        "#print(Input_data)\n",
        "#print (Input_data.shape)\n",
        "#print (Input_data.size)\n",
        "#!kill -9 -1\n",
        "show_samples(Input_data, OUTPUT_DIR + \"inputs\", 0)\n",
        "with tf.Graph().as_default():\n",
        "    train(get_batches(Input_data), Input_data.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "49152\n",
            "(49152,)\n",
            "[0.16079738 0.29327663 0.66893298 ... 0.18377068 0.0285026  0.14660189]\n",
            "0\n",
            "49152\n",
            "(49152,)\n",
            "[0.69857421 0.26228362 0.86626495 ... 0.17331725 0.36786205 0.18640001]\n",
            "0\n",
            "49152\n",
            "(49152,)\n",
            "[0.09912816 0.16211029 0.17020043 ... 0.23511541 0.20252304 0.09195335]\n",
            "0\n",
            "49152\n",
            "(49152,)\n",
            "[0.06323238 0.17502199 0.36962061 ... 0.12328078 0.2076963  0.31267433]\n",
            "0\n",
            "49152\n",
            "(49152,)\n",
            "[0.20428506 0.6692     0.06223295 ... 0.1775365  0.30418786 0.14840713]\n",
            "0\n",
            "49152\n",
            "(49152,)\n",
            "[0.1539048  0.65985379 0.04349385 ... 0.40004325 0.45789901 0.6979511 ]\n",
            "WARNING:tensorflow:From <ipython-input-76-6a171d8d9e59>:5: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From <ipython-input-76-6a171d8d9e59>:16: conv2d_transpose (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2DTranspose` instead.\n",
            "WARNING:tensorflow:From <ipython-input-76-6a171d8d9e59>:20: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
            "WARNING:tensorflow:From <ipython-input-77-5b9e48b97d85>:11: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2D` instead.\n",
            "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "1\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "1\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "1\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "1\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "1\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "2\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "2\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "2\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "2\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "2\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "3\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "3\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "3\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "3\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "3\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "4\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "4\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "4\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "4\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "4\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "5\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "5\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "5\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "5\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "5\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "6\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "6\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "6\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "6\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "6\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "7\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "7\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "7\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "7\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "7\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "8\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "8\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "8\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "8\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "8\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "9\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "9\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "9\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "9\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "9\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "10\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "10\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "10\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "10\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "10\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "11\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "11\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "11\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "11\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "11\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "12\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "12\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "12\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "12\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "12\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "13\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "13\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "13\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "13\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "13\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "14\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "14\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "14\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "14\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "14\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "15\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "15\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "15\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "15\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "15\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "16\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "16\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "16\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "16\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "16\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "17\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "17\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "17\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "17\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "17\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "18\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "18\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "18\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "18\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "18\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "19\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "19\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "19\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "19\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "19\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "20\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "20\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "20\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "20\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "21\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "21\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "21\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "21\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "21\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "22\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "22\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "22\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "22\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "22\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "23\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "23\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "23\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "23\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "23\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "24\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "24\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "24\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "24\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "24\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "25\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "25\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "25\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "25\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "25\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "26\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "26\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "26\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "26\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "26\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "27\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "27\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "27\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "27\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "27\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "28\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "28\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "28\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "28\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n",
            "28\n",
            "49152\n",
            "(49152,)\n",
            "[127 127 127 ... 127 127 127]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}